{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cf04e5-5589-468f-9c84-e82e8132cf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in ./vitenv/lib/python3.12/site-packages (2025.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8421dbd2-3f24-45fc-b8c6-19522b469fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-9sid1qh3/unsloth_14e7ce6ddd2345e2b014b7b82937b2cc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-9sid1qh3/unsloth_14e7ce6ddd2345e2b014b7b82937b2cc\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 6ac4e2e36f2f8bd0bc63a6eb85afa7097948ff3d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: xformers==0.0.27 in ./vitenv/lib/python3.12/site-packages (0.0.27)\n",
      "Requirement already satisfied: trl in ./vitenv/lib/python3.12/site-packages (0.19.0)\n",
      "Requirement already satisfied: peft in ./vitenv/lib/python3.12/site-packages (0.16.0)\n",
      "Requirement already satisfied: accelerate in ./vitenv/lib/python3.12/site-packages (1.8.1)\n",
      "Requirement already satisfied: bitsandbytes in ./vitenv/lib/python3.12/site-packages (0.46.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"unsloth @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers==0.0.27\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55618bc0-fc19-41ab-a1c9-d7917b533faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: xformers 0.0.27\n",
      "Uninstalling xformers-0.0.27:\n",
      "  Successfully uninstalled xformers-0.0.27\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Collecting xformers\n",
      "  Downloading https://download.pytorch.org/whl/cu126/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: numpy in ./vitenv/lib/python3.12/site-packages (from xformers) (2.1.3)\n",
      "Requirement already satisfied: torch==2.7.1 in ./vitenv/lib/python3.12/site-packages (from xformers) (2.7.1+cu126)\n",
      "Requirement already satisfied: filelock in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (4.14.0)\n",
      "Requirement already satisfied: setuptools in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in ./vitenv/lib/python3.12/site-packages (from torch==2.7.1->xformers) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./vitenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./vitenv/lib/python3.12/site-packages (from jinja2->torch==2.7.1->xformers) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu126/xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers\n",
      "Successfully installed xformers-0.0.31.post1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall xformers -y\n",
    "!pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87dd40e5-75c5-49e1-aace-a3916d4fe573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 10:43:57.611007: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-09 10:43:57.622353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752038037.635738 3459793 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752038037.639590 3459793 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752038037.650113 3459793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752038037.650128 3459793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752038037.650129 3459793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752038037.650130 3459793 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-09 10:43:57.654114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096\n",
    "dtype = None\n",
    "load_in_4bit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f001b5d-e0fb-482f-9c43-caa825e538a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.1: Fast Llama patching. Transformers: 4.53.1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.19 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = torch.bfloat16,\n",
    "    device_map = {\"\": torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad75347-e428-4f26-b589-93041c8c2544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.7.1 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\",\"v_proj\", \"o_proj\", \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c8e561-abb5-48e7-be53-47578c703faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"converted_train_merged.jsonl\", split=\"train\") ###replace with your training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c8687ce-c60b-4eb7-aaa0-1b43f5e9db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {\n",
    "    \" A\": 0.4,\n",
    "    \" B\": 0.4,\n",
    "    \" C\": 0.4,\n",
    "    \" D\": 0.5\n",
    "}\n",
    "\n",
    "def compute_sample_weight(example):\n",
    "    weight = 1.0  # default weight\n",
    "\n",
    "    try:\n",
    "        answer = example[\"conversations\"][-1][\"value\"].strip()\n",
    "        token = f\" {answer}\"  # prepend space to match tokenizer output\n",
    "        weight = class_weights.get(token, 1.0)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not extract answer from: {example['conversations']}\")\n",
    "        # Keep default weight\n",
    "\n",
    "    example[\"weight\"] = weight\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2128c56-afd1-4164-b522-12923e17f4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bcf50783854fdcbc554fc733435c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset.map(compute_sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "362eb67c-5931-42e2-b3f2-562d1afff946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '86b409a8-9790-48a7-b90f-ccf60a7417b9',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': 'Question: Most common chronic ahritis seen in children ?\\n\\nOptions:\\nA. JRA\\nB. Rheumatic ahritis\\nC. Rheumatic fever\\nD. Sepsis\\n\\nAnswer with the correct option letter only.'},\n",
       "  {'from': 'gpt', 'value': 'A'}],\n",
       " 'weight': 0.4}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d46c9a6f-8605-4cfb-aa79-5adcfda99116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\",\n",
    "    mapping = {\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "\n",
    "    }\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    \n",
    "    # Generate plain text from chat template\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo, tokenize=False, add_generation_prompt=False\n",
    "        ) for convo in convos\n",
    "    ]\n",
    "    \n",
    "    # Tokenize the text batch\n",
    "    tokenized = tokenizer(texts, truncation=True)\n",
    "\n",
    "    # Pass along weights (should be a list of floats with same length as batch)\n",
    "    tokenized[\"weight\"] = examples[\"weight\"]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b868e93-ddcb-4a4e-b65c-e415f15395e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c787099aa34c65af69c332e196cfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapped_dataset = train_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9e2cb4d-437a-4ec8-97c9-ab545fdc8c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '86b409a8-9790-48a7-b90f-ccf60a7417b9',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': 'Question: Most common chronic ahritis seen in children ?\\n\\nOptions:\\nA. JRA\\nB. Rheumatic ahritis\\nC. Rheumatic fever\\nD. Sepsis\\n\\nAnswer with the correct option letter only.'},\n",
       "  {'from': 'gpt', 'value': 'A'}],\n",
       " 'weight': 0.4,\n",
       " 'input_ids': [32010,\n",
       "  894,\n",
       "  29901,\n",
       "  7849,\n",
       "  3619,\n",
       "  17168,\n",
       "  293,\n",
       "  21023,\n",
       "  768,\n",
       "  275,\n",
       "  3595,\n",
       "  297,\n",
       "  4344,\n",
       "  1577,\n",
       "  13,\n",
       "  13,\n",
       "  5856,\n",
       "  29901,\n",
       "  13,\n",
       "  29909,\n",
       "  29889,\n",
       "  435,\n",
       "  4717,\n",
       "  13,\n",
       "  29933,\n",
       "  29889,\n",
       "  390,\n",
       "  354,\n",
       "  398,\n",
       "  2454,\n",
       "  21023,\n",
       "  768,\n",
       "  275,\n",
       "  13,\n",
       "  29907,\n",
       "  29889,\n",
       "  390,\n",
       "  354,\n",
       "  398,\n",
       "  2454,\n",
       "  1238,\n",
       "  369,\n",
       "  13,\n",
       "  29928,\n",
       "  29889,\n",
       "  922,\n",
       "  567,\n",
       "  275,\n",
       "  13,\n",
       "  13,\n",
       "  22550,\n",
       "  411,\n",
       "  278,\n",
       "  1959,\n",
       "  2984,\n",
       "  5497,\n",
       "  871,\n",
       "  29889,\n",
       "  32007,\n",
       "  32001,\n",
       "  319,\n",
       "  32007],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0d9af68-12c0-4f43-beb7-754af8f345da",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsloth_template = \"\"\"\n",
    "{{ bos_token }}\n",
    "You are a medical assistant answering multiple choice questions.\\n\n",
    "{% for message in messages %}\n",
    "    {% if message['from'] == 'human' %}\n",
    "        {{ '### Question:\\n' + message['value'] + '\\n' }}\n",
    "    {% else %}\n",
    "        {{ '### Answer: ' + message['value'] + eos_token + '\\n' }}\n",
    "    {% endif %}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "unsloth_eos_token = \"eos_token\"\n",
    "\n",
    "if False:\n",
    "    tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = unsloth_template,\n",
    "    eos_token = unsloth_eos_token,\n",
    "    mapping = {\n",
    "        \"role\": \"from\",\n",
    "        \"content\": \"value\",\n",
    "        \"user\": \"human\",\n",
    "        \"assistant\": \"gpt\"\n",
    "    },\n",
    "    map_eos_token = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fac1a30-d1e2-47cb-8c6a-39c485bc21f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train-09947',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': 'Question: A 15-year-old boy presents with sudden onset right sided weakness of his arm and face and difficulty speaking. He denies any problems with hearing or comprehension. The patient has no history of chest pain, hypertension, or diabetes mellitus. No significant past medical history. The patient is afebrile, and vital signs are within normal limits. On physical examination, the patient is thin, with long arms and slender fingers. There is a right-sided facial droop present. Ophthalmic examination reveals a dislocated lens in the right eye. Strength is 3 out of 5 in the right upper extremity, and there is a positive Babinski reflex on the right. The CT scan of the head shows no evidence of hemorrhage. Laboratory findings are significant for increased concentrations of a metabolic intermediate in his serum and urine. Which of the following enzymes is most likely deficient in this patient?\\n\\nOptions:\\nA. Phenylalanine hydroxylase\\nB. Homogentisate oxidase\\nC. Cystathionine synthase\\nD. Branched-chain ketoacid dehydrogenase\\n\\nAnswer with the correct option letter only.'},\n",
       "  {'from': 'gpt', 'value': 'C'}],\n",
       " 'weight': 0.4,\n",
       " 'input_ids': [32010,\n",
       "  894,\n",
       "  29901,\n",
       "  319,\n",
       "  29871,\n",
       "  29896,\n",
       "  29945,\n",
       "  29899,\n",
       "  6360,\n",
       "  29899,\n",
       "  1025,\n",
       "  8023,\n",
       "  22981,\n",
       "  411,\n",
       "  8327,\n",
       "  373,\n",
       "  842,\n",
       "  1492,\n",
       "  269,\n",
       "  2618,\n",
       "  8062,\n",
       "  2264,\n",
       "  310,\n",
       "  670,\n",
       "  5075,\n",
       "  322,\n",
       "  3700,\n",
       "  322,\n",
       "  14656,\n",
       "  13590,\n",
       "  29889,\n",
       "  940,\n",
       "  972,\n",
       "  583,\n",
       "  738,\n",
       "  4828,\n",
       "  411,\n",
       "  22514,\n",
       "  470,\n",
       "  15171,\n",
       "  2673,\n",
       "  29889,\n",
       "  450,\n",
       "  16500,\n",
       "  756,\n",
       "  694,\n",
       "  4955,\n",
       "  310,\n",
       "  521,\n",
       "  342,\n",
       "  6788,\n",
       "  29892,\n",
       "  7498,\n",
       "  10700,\n",
       "  2673,\n",
       "  29892,\n",
       "  470,\n",
       "  652,\n",
       "  370,\n",
       "  10778,\n",
       "  286,\n",
       "  514,\n",
       "  277,\n",
       "  375,\n",
       "  29889,\n",
       "  1939,\n",
       "  7282,\n",
       "  4940,\n",
       "  16083,\n",
       "  4955,\n",
       "  29889,\n",
       "  450,\n",
       "  16500,\n",
       "  338,\n",
       "  2511,\n",
       "  774,\n",
       "  13816,\n",
       "  29892,\n",
       "  322,\n",
       "  27131,\n",
       "  18906,\n",
       "  526,\n",
       "  2629,\n",
       "  4226,\n",
       "  13071,\n",
       "  29889,\n",
       "  1551,\n",
       "  9128,\n",
       "  4392,\n",
       "  3381,\n",
       "  29892,\n",
       "  278,\n",
       "  16500,\n",
       "  338,\n",
       "  16835,\n",
       "  29892,\n",
       "  411,\n",
       "  1472,\n",
       "  10188,\n",
       "  322,\n",
       "  2243,\n",
       "  1581,\n",
       "  23915,\n",
       "  29889,\n",
       "  1670,\n",
       "  338,\n",
       "  263,\n",
       "  1492,\n",
       "  29899,\n",
       "  29879,\n",
       "  2618,\n",
       "  2258,\n",
       "  1455,\n",
       "  4441,\n",
       "  459,\n",
       "  2198,\n",
       "  29889,\n",
       "  438,\n",
       "  561,\n",
       "  386,\n",
       "  284,\n",
       "  13076,\n",
       "  4392,\n",
       "  3381,\n",
       "  10320,\n",
       "  1338,\n",
       "  263,\n",
       "  766,\n",
       "  28809,\n",
       "  301,\n",
       "  575,\n",
       "  297,\n",
       "  278,\n",
       "  1492,\n",
       "  10977,\n",
       "  29889,\n",
       "  3767,\n",
       "  1477,\n",
       "  338,\n",
       "  29871,\n",
       "  29941,\n",
       "  714,\n",
       "  310,\n",
       "  29871,\n",
       "  29945,\n",
       "  297,\n",
       "  278,\n",
       "  1492,\n",
       "  7568,\n",
       "  9413,\n",
       "  537,\n",
       "  29892,\n",
       "  322,\n",
       "  727,\n",
       "  338,\n",
       "  263,\n",
       "  6374,\n",
       "  14525,\n",
       "  262,\n",
       "  2574,\n",
       "  2143,\n",
       "  2506,\n",
       "  373,\n",
       "  278,\n",
       "  1492,\n",
       "  29889,\n",
       "  450,\n",
       "  26637,\n",
       "  12812,\n",
       "  310,\n",
       "  278,\n",
       "  2343,\n",
       "  3697,\n",
       "  694,\n",
       "  10757,\n",
       "  310,\n",
       "  9736,\n",
       "  272,\n",
       "  19046,\n",
       "  482,\n",
       "  29889,\n",
       "  16715,\n",
       "  7606,\n",
       "  1284,\n",
       "  886,\n",
       "  526,\n",
       "  7282,\n",
       "  363,\n",
       "  11664,\n",
       "  14953,\n",
       "  800,\n",
       "  310,\n",
       "  263,\n",
       "  1539,\n",
       "  19388,\n",
       "  293,\n",
       "  19697,\n",
       "  297,\n",
       "  670,\n",
       "  724,\n",
       "  398,\n",
       "  322,\n",
       "  5065,\n",
       "  457,\n",
       "  29889,\n",
       "  8449,\n",
       "  310,\n",
       "  278,\n",
       "  1494,\n",
       "  427,\n",
       "  14022,\n",
       "  267,\n",
       "  338,\n",
       "  1556,\n",
       "  5517,\n",
       "  822,\n",
       "  293,\n",
       "  993,\n",
       "  297,\n",
       "  445,\n",
       "  16500,\n",
       "  29973,\n",
       "  13,\n",
       "  13,\n",
       "  5856,\n",
       "  29901,\n",
       "  13,\n",
       "  29909,\n",
       "  29889,\n",
       "  1963,\n",
       "  264,\n",
       "  2904,\n",
       "  284,\n",
       "  273,\n",
       "  457,\n",
       "  17546,\n",
       "  29916,\n",
       "  2904,\n",
       "  559,\n",
       "  13,\n",
       "  29933,\n",
       "  29889,\n",
       "  15089,\n",
       "  468,\n",
       "  296,\n",
       "  275,\n",
       "  403,\n",
       "  19100,\n",
       "  333,\n",
       "  559,\n",
       "  13,\n",
       "  29907,\n",
       "  29889,\n",
       "  315,\n",
       "  858,\n",
       "  493,\n",
       "  291,\n",
       "  457,\n",
       "  14710,\n",
       "  559,\n",
       "  13,\n",
       "  29928,\n",
       "  29889,\n",
       "  25889,\n",
       "  287,\n",
       "  29899,\n",
       "  14153,\n",
       "  413,\n",
       "  10896,\n",
       "  562,\n",
       "  333,\n",
       "  316,\n",
       "  29882,\n",
       "  11279,\n",
       "  1885,\n",
       "  559,\n",
       "  13,\n",
       "  13,\n",
       "  22550,\n",
       "  411,\n",
       "  278,\n",
       "  1959,\n",
       "  2984,\n",
       "  5497,\n",
       "  871,\n",
       "  29889,\n",
       "  32007,\n",
       "  32001,\n",
       "  315,\n",
       "  32007],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da844259-eca3-445b-9448-c2380bb3a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'conversations', 'weight', 'input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(mapped_dataset[0].keys())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a94dba0e-26c8-4ac9-b38f-1cab2e124542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2054\n"
     ]
    }
   ],
   "source": [
    "print(mapped_dataset[6][\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "774aaa5f-d658-42eb-83b0-4d3cd06bc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "mapped_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"weight\"])\n",
    "\n",
    "class DataCollatorWithWeights(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer, mlm=False)\n",
    "        \n",
    "def __call__(self, examples):\n",
    "    batch = super().__call__(examples)\n",
    "    weights = []\n",
    "    for ex in examples:\n",
    "        weights.append(ex[\"weight\"])\n",
    "    batch[\"weight\"] = torch.tensor(weights, dtype=torch.float)\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87677815-1223-4d0a-b60a-6d3b0eb07c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleWeightedSFT(SFTTrainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights  # tensor of size [vocab_size] or [num_classes]\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "        labels = inputs[\"labels\"]\n",
    "        weights = inputs.data.get(\"weight\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Shift tokens for LM loss\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        attention_mask = inputs[\"attention_mask\"][..., 1:].contiguous()\n",
    "\n",
    "        # Raw loss (no reduction)\n",
    "        token_loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            reduction='none'\n",
    "        ).view(shift_labels.size())\n",
    "\n",
    "              # Mask out padding\n",
    "        token_loss = token_loss * attention_mask\n",
    "        \n",
    "        # Sum per sequence\n",
    "        loss_per_sample = token_loss.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "        \n",
    "        # Apply per-sample weight here\n",
    "        if \"weight\" in inputs:\n",
    "            sample_weights = inputs[\"weight\"].to(loss_per_sample.device)\n",
    "            loss_per_sample = loss_per_sample * sample_weights\n",
    "        \n",
    "        # Final loss\n",
    "        final_loss = loss_per_sample.mean()\n",
    "        \n",
    "        return (final_loss, outputs) if return_outputs else final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "25e02a32-7351-4f9c-ba7f-eedb26612039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SimpleWeightedSFT(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset= mapped_dataset,\n",
    "    data_collator=DataCollatorWithWeights(tokenizer=tokenizer),\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "686557fc-b3ed-489e-9b4a-4f42206069c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 164,702 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 29,884,416 of 3,850,963,968 (0.78% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.744100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.485000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.123600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.946200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.896800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.634800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.843600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.632800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.784300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.620400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.910900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.104100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc0762e6-2b40-4cd7-85a7-cff836ff039c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model_v5/tokenizer_config.json',\n",
       " 'lora_model_v5/special_tokens_map.json',\n",
       " 'lora_model_v5/chat_template.jinja',\n",
       " 'lora_model_v5/tokenizer.model',\n",
       " 'lora_model_v5/added_tokens.json',\n",
       " 'lora_model_v5/tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model_v5\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model_v5\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46478585-1bdb-452f-84fa-01f802f95c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combined examples: 300\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "paths = [\"test_easy.jsonl\", \"test_medium.jsonl\", \"test_hard.jsonl\"]\n",
    "\n",
    "combined_test_data = []\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            combined_test_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Total combined examples: {len(combined_test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54501cef-3c1d-4d36-88b7-209dfc7c8f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Generally accepted indications for mechanical ventilatory support include\\n\\nA. PaO2 of less than 70 kPa and PaCO2 of greater than 50 kPa while breathing room air\\nB. Alveolar-arterial oxygen tension difference of 150 kPa while breathing 100% O2\\nC. Vital capacity of 40-60 mL/kg\\nD. Respiratory rate greater than 35 breaths/min\\n\\nAnswer with the correct option (A/B/C/D) only. A']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": (\n",
    "            \"Generally accepted indications for mechanical ventilatory support include\\n\\n\"\n",
    "            \"A. PaO2 of less than 70 kPa and PaCO2 of greater than 50 kPa while breathing room air\\n\"\n",
    "            \"B. Alveolar-arterial oxygen tension difference of 150 kPa while breathing 100% O2\\n\"\n",
    "            \"C. Vital capacity of 40-60 mL/kg\\n\"\n",
    "            \"D. Respiratory rate greater than 35 breaths/min\\n\\n\"\n",
    "            \"Answer with the correct option (A/B/C/D) only.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e7b52a3b-6b20-400d-9b18-62485665a9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:20<00:00, 14.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"phi-3\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "def format_question(item):\n",
    "    q = item[\"question\"]\n",
    "    opts = item[\"options\"]\n",
    "    opt_str = \"\\n\".join([f\"{k}. {opts[k]}\" for k in sorted(opts)])\n",
    "    return (\n",
    "        f\"{q}\\n\\n\"\n",
    "        f\"{opt_str}\\n\\n\"\n",
    "        \"Answer with the correct option (A/B/C/D) only.\"\n",
    "    )\n",
    "\n",
    "def get_answer(messages):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=1,         # Only want one token (A/B/C/D)\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    generated_ids = outputs[0][input_ids.shape[1]:]  # Get only new tokens\n",
    "    decoded = tokenizer.decode(generated_ids, skip_special_tokens=True).strip().upper()\n",
    "\n",
    "    if decoded in [chr(c) for c in range(ord(\"A\"), ord(\"Z\") + 1)]:\n",
    "        return decoded\n",
    "    else: None\n",
    "\n",
    "# Run through test data\n",
    "results = []\n",
    "for item in tqdm(combined_test_data):\n",
    "    prompt = format_question(item)\n",
    "    messages = [{\"from\": \"human\", \"value\": prompt}]\n",
    "    answer = get_answer(messages)\n",
    "    results.append({\"id\": item[\"id\"], \"answer\": answer})\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"submission_5.csv\", index=False)\n",
    "print(\"✅ Saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ee389-cfb1-4e1e-b251-f9b3fc8d508e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
